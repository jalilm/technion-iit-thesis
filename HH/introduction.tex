\section{Introduction}
\label{sec:introduction}

%\subsection{Background}
Modern Infrastructure as a Service (IaaS) providers rely on various network protocols to provide efficient service management abilities. These protocols, as well, rely heavily on the ability to provide correct and efficient network monitoring that generate the needed statistics. Such protocols belong to various service management sub-domains such as traffic engineering~\cite{microte}, anomaly detection~\cite{Moraney2016}, load balancing~\cite{networkLB}, NFV placements~\cite{NFV-dor} and many more.

The ability to provide accurate per flow statistics, while greatly important for service management protocols, is often impractical due to the high number of active flows and the limited on-chip memory needed to keep such counters. Furthermore, no remediation is expected in the near future; more flows are flowing through networks than before, line rate is ever increasing requiring counters with more width and more monitoring applications are running, requiring their own portion of the available memory. 

Given these limitations, it is important to consider designing and implementing efficient practical algorithms for various monitoring tasks~\cite{moraney2018, moraney2020}. We consider an algorithm to be an efficient practical monitoring algorithm if (1) it provides important monitoring data, (2) performs $O(1)$ operations per packet to cope with line rate, (3) uses a constant limited amount of memory that is much smaller than and not related to the number of active flows, and finally (4) implementable on memory schemes of off-the-shelf network gear.

Many of the current state-of-the-art and most notable monitoring schemes, require a non-constant amount of memory depending either on the traces size or in inverse relation to the guaranteed accuracy~\cite{slidingHH,metwally2005efficient,SpaceSaving,Ben-Basat2017}. In such schemes, the required memory is usually of size $o(\frac{1}{\epsilon})$ where $\epsilon$ is the guaranteed accuracy. This notation  hides the fact that in many practical use-cases their memory requirement is  higher than what is available in the network device.

With the increasing amount of various monitoring applications running concurrently on network devices, the approach of setting accuracy parameters per application and only then count for their required memory is not operational. This approach makes it hard for network operators to meet the memory constraint by prioritizing applications and optimizing their accuracy parameters. The correct approach to manage on-device monitoring applications is to set hard constraints on their memory footprint and only then set the desired accuracy parameters while slightly compromise the quality of the monitoring results according to the available resources. 

The quality of the monitoring results is commonly measured via two metrics, \textit{Recall} and \textit{False Positive Rate} (FPR). Recall (sometimes called also Detection Rate) measures the proportion of positives items that are correctly identified while the FPR measures the proportion of the none-positive items that are reported as positive (i,e,. misidentified) from the set of reported items.  Describing the quality of the monitoring data without reporting both metrics is not complete. Since, while high recall algorithms are preferred, this should not come at a a price of a very high FPR. An extreme example of such algorithm is the one that reports all flows as positive, this algorithm will have a recall of $100\%$ but also almost a $100\%$ FPR hindering its output useless.

Having on-device monitoring algorithms that support line rate is of high importance, no network operator in right mind would consider reducing network capacity due to the deployment of monitoring algorithms on devices. The guarantee of line rate performance usually manifests in having $O(1)$ operations per packet, the motivation is if the algorithm performs only ``light weight" operation per arriving packet it would meet the line rate requirement.

Many monitoring algorithms maintain complex schemes and data structures that require once in a while perform a ``heavy" operation (for example a re-ordering of the data scheme). Algorithms of this type claim to support line rate while having an amortized $O(1)$ operation by proving that this ``heavy" operation happens once in many packets and when its cost is split among all the relevant packets it would add a constant negligible effect.

This point of view on line rate operation is in many cases impractical. While truly it is an amortized $O(1)$ operation, during such operations, arriving packets must be stored to be later processed and this could affect the devices available buffers and affects its performance. Thus, one of our motivations is to design a scheme that performs worst-case $O(1)$ operation.

\subsection{Related Work}

While the problem of detecting Heavy Hitter Flows, is a classical monitoring problem that is easily solvable given enough memory and had been addressed heavily in the literature~\cite{fang1999computing,gilbert2001surfing,karp2003simple,Demaine2002,slidingHH,basat2017optimal,zadnik2011evolution}, little thought was given on the practicality of the deployment of the proposed solutions.

The state-of-the-art algorithm to detect HH flows was introduced in~\cite{basat2017optimal}. In this paper the authors introduced ,IM-SUM, an algorithm which solves an $\epsilon$-approximation of the \textit{frequency estimation} problem and a $(\phi, \epsilon)$-approximation of the the HH problem. The algorithm holds two monitoring tables, a passive and an active tables, then periodically replaces their roles. The motivation is that the passive table holds the ``biggest" flows and the active holds the most ``recent" flows. When the passive table gets full a ``heavy" maintenance operation is performed, that calculates the $\lfloor\frac{1}{\epsilon}\rfloor^{th}$ largest value in the passive table, drops all entries smaller than it and moves larger entries to the active table. Furthermore, a variable $q$ holds the value of the last calculated $\lfloor\frac{1}{\epsilon}\rfloor^{th}$ largest value and it functions as the estimation of flows not in any of the tables.

The main drawbacks of this algorithm are: (1) it performs a ``heavy" maintenance operation that makes its performance an amortized $O(1)$ operation, (2) it requires an amount of memory that is proportional to the inverse of the accuracy parameter $O(\frac{1}{\epsilon})$ and (3) its approximation is of the form $N(\phi -\epsilon)$ where $N$ is the number of packets, $\phi$ is the threshold and $\epsilon$ is the accuracy parameter. This means that slack given to the approximation is $\epsilon N$ which is proportional to the amount of traffic processed by the algorithm.

In~\cite{CEDAR}, the authors introduced the \textit{CEDAR} algorithm that decouples the stored flows identifiers from their estimation values by using a constant size table to store shared estimators. The algorithm holds two tables, the first one have an entry per each flow which holds the flow's identifier and an integer. This integer as an index to the second table pointing to the flow's current estimation value. Thus, the second table is a shared estimators table, where estimators values are constructed in a manner to provide unbiased estimations while achieving minimal maximal relative estimation error. When a new flow arrives, it enters the flows table with an index of $0$, meaning its initial estimation is $v_0=0$. On each arrival of a packet from a previously seen flow, the algorithm extracts its current estimation, $v_i$, and the value of the next estimator $v_{i+1}$ and increases the estimation of the flow to be $v_{i+1}$ with probability $\frac{1}{v_{i+1}-v_{i}}$.

\subsection{This paper}
The main drawback of ~\cite{CEDAR} is that it maintains an entry per each active flow in the system and thus unpractical as a HH algorithm.
In this paper, we devise an algorithm that takes benefit of the constant amount of estimators in the CEDAR algorithm while keeping the needed memory also constant. This can be achieved since we only care about the size of large flows (HH candidate) and we do not need to maintain an estimator for each small flow. The main difficulty is thus identifying the flows that need to be monitored; this is done by looking at flows that have frequent appearance.  We keep in a table called   \sfa\ all recent flows, and when a new packet from one of these flows arrives before the flow is evicted from the table we mark this flow as a HH candidate and advance it to the \cs\ where its frequency is being measured by the shared estimators. 

The main contribution of this paper is that we present a practical heavy hitters detection algorithm that requires a constant amount of memory (not related to the number of flows or the number of packets) and performs at most $O(1)$ operation per packet to keep with line rate speed. Furthermore, we compare it to state-of-the-art monitoring solutions on real internet traces, showing a superior performance where the allocated memory is less than $1MB$.


In Section~\ref{sec:architecture} we describe the architecture and the data structures of our algorithm, the \cs\ algorithm, and then in Section~\ref{sec:theory}, we analyze  the possible source of errors. In Section~\ref{sec:improvemnts} we present a less robust variant of our algorithm, the \eb\ algorithm, that performs better when the trace is heavy tailed. Then in Section~\ref{sec:evaluation} we evaluate the \cs\ algorithm performances and compare it to state-of-art-algorithm, and finally we conclude in Section~\ref{sec:conclusions}.
