\section{Introduction}
\label{sec:introduction}

While the problem of detecting Heavy Hitter Flows, is a classical monitoring problem that is easily solvable given enough memory and had been addressed heavily in the literature~\cite{fang1999computing,gilbert2001surfing,karp2003simple,Demaine2002,slidingHH,basat2017optimal,zadnik2011evolution}, little thought was given on the practicality of the deployment of the proposed solutions.

The state-of-the-art algorithm to detect HH flows was introduced in~\cite{basat2017optimal}. In this paper the authors introduced, $IM-SUM$, an algorithm which solves an $\epsilon$-approximation of the \textit{frequency estimation} problem and a $(\phi, \epsilon)$-approximation of the HH problem. The algorithm holds two monitoring tables, a passive and an active tables, then periodically replaces their roles. The motivation is that the passive table holds the ``biggest" flows and the active holds the most ``recent" flows. When the passive table gets full a ``heavy" maintenance operation is performed, that calculates the $\lfloor\frac{1}{\epsilon}\rfloor^{th}$ largest value in the passive table, drops all entries smaller than it and moves larger entries to the active table. Furthermore, a variable $q$ holds the value of the last calculated $\lfloor\frac{1}{\epsilon}\rfloor^{th}$ largest value and it functions as the estimation of flows not in any of the tables.

The main drawbacks of this algorithm are: (1) it performs a ``heavy" maintenance operation that makes its performance an amortized $O(1)$ operation, (2) it requires an amount of memory that is proportional to the inverse of the accuracy parameter $O(\frac{1}{\epsilon})$ and (3) its approximation is of the form $N(\phi -\epsilon)$ where $N$ is the number of packets, $\phi$ is the threshold and $\epsilon$ is the accuracy parameter. This means that slack given to the approximation is $\epsilon N$ which is proportional to the amount of traffic processed by the algorithm.

In this chapter we devised a practical algorithm for the problem of detecting Heavy Hitters Flows, by combining ideas from~\cite{sampleAndHold} and~\cite{CEDAR}. The authors of~\cite{sampleAndHold}, presented an algorithm called ``Sample \& Hold" which holds an exact counter for each sampled flow. This way, it assures that the estimation of the sampled flow is quite accurate, since after the first sampled packet of the flow the algorithms measures all of its packets. This means that the only error in the estimation of the flow's frequency comes from packets there were not sampled prior to the first sampled packet. They showed then when oversampling by a factor of $O$, i.e. sampling with probability of $p=O/(\phi N)$, the probability of missing a flow of size $\phi N$ is approximately $e^{-O}$ and that the relative error is estimating such flow is $\sqrt{2-p}/O$. The main drawbacks of ``Sample \& Hold", are (i) that it requires oversampling which yields in higher memory usage, (ii) performs a per-packet operation for ``held" flows, and (iii) that the sampling is weight based, i.e. done on the packet size which complicates its logic.

In~\cite{CEDAR}, the authors introduced the \textit{CEDAR} algorithm that decouples the stored flows identifiers from their estimation values by using a constant size table to store shared estimators. The algorithm holds two tables, the first one have an entry per each flow which holds the flow's identifier and an integer. This integer as an index to the second table pointing to the flow's current estimation value. Thus, the second table is a shared estimators table, where estimators values are constructed in a manner to provide unbiased estimations while achieving minimal maximal relative estimation error. When a new flow arrives, it enters the flows table with an index of $0$, meaning its initial estimation is $v_0=0$. On each arrival of a packet from a previously seen flow, the algorithm extracts its current estimation, $v_i$, and the value of the next estimator $v_{i+1}$ and increases the estimation of the flow to be $v_{i+1}$ with probability $\frac{1}{v_{i+1}-v_{i}}$. The main drawback of ~\cite{CEDAR} is that it maintains an entry per each active flow in the system and thus unpractical as a HH algorithm.

Given these algorithms and their drawbacks, we devised an algorithm that builds on top of the ``Sample \& Hold" concept without being limited to the weighted sampling requirement, the per-packet operation and the oversampling requirement. The algorithm also takes advantage of constant amount of estimators in the CEDAR algorithm while keeping the needed memory also constant, and this can be achieved since we only care about the size of large flows (HH candidate) and we do not need to maintain an estimator for each small flow. The main difficulty is identifying the flows that need to be monitored; this is done by looking at flows that have frequent appearance. We keep in a table called \sfa\ all recent flows, and when a new packet from one of these flows arrives before the flow is evicted from the table we mark this flow as a HH candidate and advance it to the \cs\ where its frequency is being measured by the shared estimators. 

The main contribution of our work in this chapter is that we present a practical heavy hitters detection algorithm that requires a constant amount of memory (not related to the number of flows, the number of packets or the oversampling parameter) and performs at most $O(1)$ operation per sampled packet to keep with line rate speed. Furthermore, we compare it to state-of-the-art monitoring solutions on real internet traces, showing a superior performance where the allocated memory is less than $1MB$.

In Section~\ref{sec:architecture} we describe the architecture and the data structures of our algorithm, the \cs\ algorithm, and then in Section~\ref{sec:theory}, we analyze  the possible source of errors. In Section~\ref{sec:improvemnts} we present a less robust variant of our algorithm, the \eb\ algorithm, that performs better when the trace is heavy tailed. Finally, in Section~\ref{sec:evaluation} we evaluate the \cs\ algorithm performances and compare it to state-of-art-algorithm.
