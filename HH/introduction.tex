\section{Introduction}
\label{sec:introduction}

While the problem of detecting Heavy Hitter Flows, is a classical monitoring problem that is easily solvable given enough memory and had been addressed heavily in the literature~\cite{fang1999computing,gilbert2001surfing,karp2003simple,Demaine2002,slidingHH,basat2017optimal,zadnik2011evolution}, little thought was given on the practicality of the deployment of the proposed solutions.

The state-of-the-art algorithm to detect HH flows was introduced in~\cite{basat2017optimal}. In this paper the authors introduced ,IM-SUM, an algorithm which solves an $\epsilon$-approximation of the \textit{frequency estimation} problem and a $(\phi, \epsilon)$-approximation of the the HH problem. The algorithm holds two monitoring tables, a passive and an active tables, then periodically replaces their roles. The motivation is that the passive table holds the ``biggest" flows and the active holds the most ``recent" flows. When the passive table gets full a ``heavy" maintenance operation is performed, that calculates the $\lfloor\frac{1}{\epsilon}\rfloor^{th}$ largest value in the passive table, drops all entries smaller than it and moves larger entries to the active table. Furthermore, a variable $q$ holds the value of the last calculated $\lfloor\frac{1}{\epsilon}\rfloor^{th}$ largest value and it functions as the estimation of flows not in any of the tables.

The main drawbacks of this algorithm are: (1) it performs a ``heavy" maintenance operation that makes its performance an amortized $O(1)$ operation, (2) it requires an amount of memory that is proportional to the inverse of the accuracy parameter $O(\frac{1}{\epsilon})$ and (3) its approximation is of the form $N(\phi -\epsilon)$ where $N$ is the number of packets, $\phi$ is the threshold and $\epsilon$ is the accuracy parameter. This means that slack given to the approximation is $\epsilon N$ which is proportional to the amount of traffic processed by the algorithm.

In~\cite{CEDAR}, the authors introduced the \textit{CEDAR} algorithm that decouples the stored flows identifiers from their estimation values by using a constant size table to store shared estimators. The algorithm holds two tables, the first one have an entry per each flow which holds the flow's identifier and an integer. This integer as an index to the second table pointing to the flow's current estimation value. Thus, the second table is a shared estimators table, where estimators values are constructed in a manner to provide unbiased estimations while achieving minimal maximal relative estimation error. When a new flow arrives, it enters the flows table with an index of $0$, meaning its initial estimation is $v_0=0$. On each arrival of a packet from a previously seen flow, the algorithm extracts its current estimation, $v_i$, and the value of the next estimator $v_{i+1}$ and increases the estimation of the flow to be $v_{i+1}$ with probability $\frac{1}{v_{i+1}-v_{i}}$.

The main drawback of ~\cite{CEDAR} is that it maintains an entry per each active flow in the system and thus unpractical as a HH algorithm.
In this chapter, we devise an algorithm that takes benefit of the constant amount of estimators in the CEDAR algorithm while keeping the needed memory also constant. This can be achieved since we only care about the size of large flows (HH candidate) and we do not need to maintain an estimator for each small flow. The main difficulty is thus identifying the flows that need to be monitored; this is done by looking at flows that have frequent appearance.  We keep in a table called   \sfa\ all recent flows, and when a new packet from one of these flows arrives before the flow is evicted from the table we mark this flow as a HH candidate and advance it to the \cs\ where its frequency is being measured by the shared estimators. 

The main contribution of our work in this chapter is that we present a practical heavy hitters detection algorithm that requires a constant amount of memory (not related to the number of flows or the number of packets) and performs at most $O(1)$ operation per packet to keep with line rate speed. Furthermore, we compare it to state-of-the-art monitoring solutions on real internet traces, showing a superior performance where the allocated memory is less than $1MB$.


In Section~\ref{sec:architecture} we describe the architecture and the data structures of our algorithm, the \cs\ algorithm, and then in Section~\ref{sec:theory}, we analyze  the possible source of errors. In Section~\ref{sec:improvemnts} we present a less robust variant of our algorithm, the \eb\ algorithm, that performs better when the trace is heavy tailed. Finally, in Section~\ref{sec:evaluation} we evaluate the \cs\ algorithm performances and compare it to state-of-art-algorithm.
