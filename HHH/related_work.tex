\section{Related Work}
Various papers had addressed the efficient detection of Hierarchical Heavy Hitters in the fields of streaming data and network monitoring. These papers dealt with many aspects of the problem, including but not limited to (1) The number of dimensions of the hierarchy, (2) Relaxation of the problem by approximation, (3) Space requirements of the algorithms, (4) per item (packet) update time, (5) Lower bound on required Space, and (6) Convergence requirements of the algorithms.

The single dimension variant of the problem was introduced
and approximated by a streaming algorithm in~\cite{HHHInStreams}.
Later, ~\cite{separator} introduced an algorithm that requires $O(H^2/\epsilon)$ space, where $H$ is the depth of the hierarchy and $\epsilon$ is the allowed relative estimation error for each single flow frequency.

Many algorithms to solve the multiple dimensions variant of the problem were proposed, each with its own properties ~\cite{HHHOnline, SpaceSaving, HHHInStreams-MultiDimension, weightedCircuits, MultiDimensionHHH, lowerboundsonMultiDimension}. The common property of these algorithms is that the depth of the multi-dimensional hierarchy is the product of each dimension.

In ~\cite{HHHOnline}, trie based algorithms were proposed that requires $O(H)$ update time, space of $O(H^2/\epsilon)$ and  maintains a special data structure to hold the trie and its dynamic expansion. More recently, another trie-based solution was introduced in ~\cite{HHHInStreams-MultiDimension}, the Full Ancestry and Partial Ancestry, that use $O(Hlog(N\epsilon)/\epsilon)$ space and requires $O(Hlog(N\epsilon))$ time per update, where $N$ is the stream length. Alternatively, ~\cite{weightedCircuits} improve the required space and update time to $O(H^{3/2}/\epsilon)$ (regardless of $N$) for the two dimensional problem.

A more efficient sketch-based algorithm with strong error and space guarantees were proposed in ~\cite{SpaceSaving}. In their approach, they utilized a copy of Space Saving Sketch~\cite{metwally2005efficient} per level, and detected the hierarchical heavy hitters by detected heavy hitters throughout all levels of the hierarchy. The algorithm requires $O(H/\epsilon)$ space and $O(H)$ update time~\footnote{In the weighed version the update time is $O(Hlog(1/\epsilon))$}.

Recently, a probabilistic version of the hierarchical heavy hitters problem was described in~\cite{ben2017constant}. Exploiting this definition, the algorithm improved the update time to $O(1)$ on the expense of requiring a convergence interval. That is, a minimal number of items has to be processed before the probabilistic guarantees of the algorithm hold. The authors argue that in practice the algorithm's output is of the same quality as the previous deterministic approaches, while the convergence interval is not a real limitation since modern networks route a continuously increasing number of packets.

DREAM~\cite{Moshref2014} is a framework for dynamically scheduling network traffic monitoring jobs (as black boxes), by allocating the amount of resources needed to maintain the predetermined accuracy of each job. When a monitoring job with a given accuracy requires more resources than  available in the switch, the framework either rejects the job or tries to perform an expensive multi-switch resource allocation to achieve the desired accuracy rates.
Such framework effectively balances two of the most critical characteristics of monitoring jobs, resources' availability and monitoring accuracy, and it is a very important and useful tool  to schedule monitoring jobs in a compatible manner.

Our paper describes a different approach to detect hierarchical heavy hitters,
while still being usable as a black box monitoring job in scheduling frameworks such as DREAM. The main trait of the approach is the practicality of deploying our algorithms on off-the-shelf network nodes while using a given number of counters (space) to detect the hierarchical heavy hitters. Furthermore, the per-packet update time of the proposed algorithms is $O(1)$ without requiring any special data-structure or any convergence interval.