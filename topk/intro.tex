\section{Introduction}
\ignore{
The rapid growth of Cloud Computing and the expansion of Infrastructure as a Service (IaaS)~\cite{Goyal2013} as the preferred solution for providing low cost IT introduce many new challenges in the field of infrastructure management. In this context, it is important to address problems related to efficient monitoring of network resources, since the monitored information is a crucial building block in any IaaS management solution.
The networked cloud environment is distributed and the system behavior depends on many parameters that belong to different elements of the network and the cloud. Thus, the monitoring process itself, i.e., the process of collecting the relevant information from the different network locations, requires a considerable amount of resources and should be optimized with respect to the overall gained value.


A typical monitoring task, which is straightforward for memory-intensive solutions, is the detection of the top-$k$ flows, i.e., identifying the $k$ flows with the highest rate in a given period of time. When sufficient memory is available, one can track the frequency of each flow and easily detect the $k$ most frequent flows. However, with memory constraints in mind, it is challenging to design monitoring algorithm for the top-$k$ problem using constant (or even sublinear) space with respect to the number of active flows or the overall traffic size.
}

Detecting the top-$k$ flows that go through a given network node has been addressed before by works in the streaming domain \cite{Demaine2002}, \cite{Metwally2005} and \cite{Ben-Basat2017}. All of these approaches require an operation per an arriving packet. In some cases, the operation can be as complicated as querying all counters (usually to calculate a minimal value) and assigning a new value to a different counter. Therefore, these algorithms are required to perform ``per packet'' complicated operation on complex data structures (such as doubly linked lists) in line rate. This makes using the results of~\cite{Demaine2002} and~\cite{Metwally2005} unsuitable for deployment on commodity network nodes.

Another drawback of the approaches in \cite{Demaine2002} and \cite{Metwally2005} is that they perform poorly on heavy-tailed traffics. In \cite{Demaine2002} on the arrival of a packet from a non-monitored flow, all counters are decremented and possibly a single counter is reassigned, while in \cite{Metwally2005} on the arrival of every non-monitored packet, the minimal counter is reassigned causing poor performance on heavy-tailed traffic. The approach in \cite{Ben-Basat2017} dealt with heavy-tailed traffic by probabilistically deciding if to reassign a counter on the arrival of a non-monitored packet, which indeed yielded better performance than \cite{Demaine2002} and \cite{Metwally2005}. The authors of \cite{Ben-Basat2017} also deal with the need to use complex data structure by assuming the existence of a d-way associative cache that supports metadata updates in the node’s hardware. The usage of such cache required $O(d)$ operation per arriving packet regardless if any counter will be reassigned or even updated by this arrival and to achieve good detection $d$ should be at least $16$. This usage of $16$-way cache comes at a cost of low precision of about $0.5$, which means that the algorithm outputs $2k$ candidate flows that contain $0.9k$ flows of the actual top-$k$.

It is important to note that these streaming algorithms aim at identifying the top-$k$ flows in terms of {\em packet rate}, regardless of the packets' sizes. While this might be relevant in some settings, the more important practical problem is identifying the top-$k$ flows in terms of bandwidth, i.e., in terms of bits per second rather than packets per second. This requires, in streaming terminology, to address the more complex {\em weighted} version of the top-$k$ problem.% and this is more complex and less accurate.

Another undesirable character of these approaches is that they require an additive slack of $N\varepsilon$, where $N$ is the number of packets and $\frac{1}{\varepsilon}$ is correlated to the number of counters. When the number of counters is not big enough for a given $N$, this additive slack may become too big to provide any meaningful information about the top-$k$ flows.

In this chapter, we take a different approach that takes advantage of flow table entries, already exist in network nodes. Since clearly, we do not have enough counters to explicitly count the packets in each active flow, we have to reconfigure the counters filter over time. However, this is done on a fixed time interval (say every second) and thus all of our operations can be handled in line rate with no need for additional data structures. Moreover, the use of the built-in counters allows us to solve both unweighted and weighted versions of the problem at the same complexity since these built-in counters can count packets and bytes at no additional cost.

To evaluate the expected effect of the complex ``per pack'' operation on the switch`s throughput, we implemented the state of the art streaming algorithms from~\cite{Ben-Basat2017} on top of an OpenvSwitch and measured the expected throughput (see Table~\ref{tbl:ovs_perf}).  With no additional monitoring algorithm, the switch could handle 1.29 MPPS (million packets per second), however when the streaming algorithms are added the throughput went down to 0.83 MPPS.  The monitoring technique presented in this chapter are implemented on hardware devices using built-in fast counters and will have a very small impact on the throughput.  However, just to be in the safe side we also implemented these algorithms in software and measured the impact of this implementation on the measured throughput. The results indicate that even in software implementation the most complex version of our techniques (called \ref{algo:HashNodeExactTop}) performs better than the best available streaming techniques. 

In general, we adopt the approach of~\cite{Moraney2016}, and assign to each counter an aggregated subset of the flows rather than a single flow. Then, at the end of each period, the values of the counters are evaluated and new subsets are assigned to the counters. Furthermore, we provide a mechanism to estimate the flow's frequency throughout several periods based on the values at the end of each period. The two main motivations behind this aggregated periodic approach are: minimizing the monitoring related traffic and avoiding interferences from low frequency flows in heavy-tailed traffic. Moreover, in this approach, the algorithms can guarantee full precision in the sense of returning exactly $k$ flows. Such favorable property allows users not to worry about choosing which $k$ flows of the output to treat as the top-$k$ flows.

We present three new techniques that dramatically improve overall performance. 
First, we introduce the basic algorithm which in contrast to~\cite{Moraney2016} focuses on several groups of the flow universe simultaneously.
Then, we suggest splitting the monitoring process into rounds and leverages the data collected from past rounds in the current round.
Finally, we suggest hashing the packets before processing them to break several unwanted properties of the traffic.

The result is a family of efficient monitoring algorithms to the top-$k$ problem which are deployable out of the box on any OpenFlow enabled network node. The algorithms use a configurable constant number of counters and guarantee that no deviation from the allocated counters will ever occur.  We evaluate the expected performance of our algorithms on real network traffic through an extensive simulation study using CAIDA’s traces~\cite{CAIDA14,CAIDA15,CAIDA2016}.  The results indicate that using only a small constant number of counters the algorithms can identify the top-$k$ flows (in terms of the amount of traffic) with very good accuracy.  For example, one can detect the top-8 flows out of thousands of active flow, using only 32 counters with an average detection rate of 87\%. 

We also compare the performance of our algorithms to RAP, the best streaming algorithm reported to date~\cite{Ben-Basat2017}. In doing so, one should be very careful in defining the appropriate way to measure counters' usage since the authors of~\cite{Ben-Basat2017} use 16-way cache and assume metadata availability. When using our algorithms to solve the top-$k$ packet rate problem, they perform almost as well as RAP (on CADIA 2015 data), and in some setting a bit better. However, when we implemented RAP and modified it to detect the weighted top-$k$ flows (in terms of bit rate), our algorithms outperform RAP by 10-15\%.

\input{topk/performancetable.tex}

The rest of this chapter is organized as follows. First, we introduce a survey of previous work, then we define the top-$k$ problem and introduce possible approximations for it. Afterward, we introduce our basic algorithm which solves the top-$k$ problem locally on a network node and suggest two improvements for it and we evaluate the suggested algorithms and compare them to the state of the art algorithm.