\section{Related Work}
Formally, the top-$k$ problem aims at finding the top-$k$ elements that have the highest frequency from a stream of elements. The space requirements for an exact solution to this problem are impractical~\cite{Charikar2004} and thus many relaxations of the original problem were proposed.

The $FindCandidateTop(S, k, l)$ problem was proposed in~\cite{Charikar2004} defining the $l$ elements among which the top-$k$ elements are concealed, with no guarantees on the rank of the remaining $l-k$ elements. While the more practical approximation $FindApproxTop(S, k, \varepsilon)$, also proposed in~\cite{Charikar2004} requires a list of $k$ elements, where every element in the list has a frequency within $(1-\varepsilon)$ of the $k^{th}$ element frequency.

To evaluate the performance of algorithms solving these approximations, two parameters were proposed~\cite{Cormode2005} (in addition to the space requirement): \textit{recall,} the number of correct elements found as a percentage of the number of actual correct elements; and the \textit{precision}, the number of correct elements found as a percentage of the entire output.

% The \textit{Frequent} algorithm proposed in~\cite{Demaine2002} outputs a list of $k$ elements with no guarantee on the elements frequency. It keeps $k$ counters to monitor $k$ elements, if a monitored element is observed, its counter is incremented, else all counters are decremented.
% If a counter reaches $0$, it is assigned the next observed element. When the algorithm terminates, the monitored elements are the candidate frequent elements. A significant contribution of this work was the proposed lightweight data structure (a doubly linked list) that can decrement all counters in $O(1)$ operations.
% The main drawback of this approach is the usage of dynamically allocated memory and pointers for the lightweight data structure, which makes difficult to implement on network nodes.

Metwally et al. proposed the \textit{Space-Saving} algorithm in~\cite{Metwally2005}. It builds on the ideas of \textit{Frequent}~\cite{Demaine2002} while improving the time complexity of looking up a value of a counter from $O(m)$ to $O(1)$. The main difference from \textit{Frequent} is that when a non-monitored item arrives, the algorithms assign it the minimal counter while preserving its value. They proved that regardless of the data distribution, \textit{Space-Saving} needs $min(|A|,\frac{N}{\varepsilon F_k})$ counters to correctly solve $FindApproxTop(S, k, \varepsilon)$. Where $A$ is the elements universe, $N$ is the size of the stream and $F_k$ is the frequency of the $k^{th}$ element. When considering high volume network traffic, the value of $|A|$ is at least $2^{32}$ while $\frac{N}{F_k}$ can be very high depending on the traffic's characteristics, thus the number of counters can be unrealistically high.

Ben-Basat et al.~\cite{Ben-Basat2017} introduced the \textit{Randomized Admission Policy (RAP)} algorithm which randomly decides if to reassign a counter to measure non-monitored items. The probability of reassigning a counter is in inverse relation to the counter's value. The motivation behind this reassignment policy is that infrequent non-monitored items will need to arrive several consecutive times to replace a monitored item. The authors acknowledge the difficulty of implementing RAP in hardware and presented a hardware-friendly variant, \textit{d-Way Randomized Admission Policy (dW-RAP)}, which performs poorer than RAP. This variant assumes the existence of $d$-way associative cache in the node's hardware, where its entries can be partitioned to store metadata (ID) and value.

Several other recent works have also focused on efficient resource-constrained flow monitoring, realizing that measurement is crucial for network management and control and even more so in the SDN domain~\cite{Moraney2016, Moshref2013, Moshref2014}. Moraney and Raz introduced in~\cite{Moraney2016} an efficient scheme to detect flow anomalies, based on a variation of the MRT algorithm presented in~\cite{Moshref2014}. The scheme used a constant number of counters, in periodic assessment and reassignment fashion, to detect anomalous flows regardless of the number of active flows.

In~\cite{Moraney2016} an efficient anomaly detection mechanism was introduced. The mechanism is based on detecting ``over-usage" in regards to flows, i.e., flows that surpass a given threshold. While such a mechanism is efficient in detecting flows that have a certain individual property, it is not usable in detecting flows that uphold a global property such as top-$k$ flow. Furthermore, the authors assumed a given application-specific threshold that defines when the local property holds. However, setting such thresholds is a non-trivial hard task since they need to capture the various aspects of the anomaly and the traffic.

The authors of~\cite{Moshref2013} concentrated on the tradeoff between the amount of available resources and the accuracy of the measurement. Note, that like many of the works in this area, they do not present an analytical framework to study this tradeoff and rather concentrated on important system related issues.
The same authors proposed more recently in~\cite{Moshref2014} a network wise dynamic monitoring system where the SDN controller dynamically configures monitoring rules in the different network elements. Such a centralized management entity can thus make use of global information in order to utilize the distributed monitoring resources (typically TCAM rules) in an efficient way, that is, getting as much precision as possible for the given monitoring resources.
